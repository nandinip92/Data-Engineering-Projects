{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "An Analysis complany wants to analyse US Immigration data.Their main focus is on the immigrations done through 'Air' travelling. \n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#import findspark\n",
    "#findspark.init()\n",
    "#import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf,col,explode,monotonically_increasing_id\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, DateType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "In this project I am going to build a data model that connects the given dataset and explore the data belongs United States and the immigration through 'Air' mode.\n",
    "#### Datasets Used\n",
    "The following datasets are included in the project.\n",
    "* **I94 Immigration Data:** This data comes from the US National Tourism and Trade Office. A data dictionary is included for working by Udacity. [This](https://travel.trade.gov/research/reports/i94/historical/2016.html) is where the data comes from. \n",
    "    * _`i94_apr16_sub.sas7bdat`_ is the original one of the files with complete data present on  Udacity disk. \n",
    "    * `'../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'` path in Udacity workspace for the file which has lot of data processing it takes lot of time on local system.\n",
    "    * `I94_SAS_Labels_Descriptions.SAS` is the Label Desctiptor file for the SAS file which has valid and invalid codes present in the I94 Immigration Data file.\n",
    "    * There's a sample file so you can take a look at the data in csv format before reading it all in. `immigration_data_sample.csv`. \n",
    "* **World Temperature Data:** This dataset came from Kaggle. You can read more about it [here](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data).\n",
    "    *  `../../data2/GlobalLandTemperaturesByCity.csv` is path for the file on Udacity workspace\n",
    "* **U.S. City Demographic Data:** This data comes from OpenSoft. You can read more about it [here](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/). - `us-cities-demographics.csv`\n",
    "* **Airport Code Table:** This is a simple table of airport codes and corresponding cities. It comes from [here](https://datahub.io/core/airport-codes#data). - `airport-codes_csv.csv`\n",
    "\n",
    "_NOTE:_ \n",
    "    1. All the `csv` data Sample Files are present in the `Datasets` Folder\n",
    "    2. Ignore the extra `immigration_data_sample.csv` has an extra column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Follwing is the code for working in Udacity Workspace\n",
    "\t\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "\n",
    "df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "\n",
    "#write SAS data to parquet\n",
    "df_spark.write.mode(\"overwrite\").partitionBy(\"i94yr\",\"i94mon\").parquet(\"Datasets_created_by_Scripts/sas_data\")\n",
    "immigration_df=spark.read.parquet(\"Datasets_created_by_Scripts/sas_data\")\n",
    "#df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "###################################################################################\n",
    "### Read in 'us-cities-demographics.csv' and  'airport-codes_csv.csv' data here ###\n",
    "###################################################################################\n",
    "\n",
    "us_cities_df = spark.read.csv(\"Datasets/us-cities-demographics.csv\",sep = \";\",header = True)\n",
    "airport_codes_df = spark.read.csv(\"Datasets/airport-codes_csv.csv\",header = True)\n",
    "\n",
    "#df = spark.read.csv(\"Datasets/immigration_data_sample.csv\")\n",
    "\n",
    "#us_cities_df.printSchema()\n",
    "#airport_codes_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#########################################################################################################\n",
    "### 1. Read in 'GlobalLandTemperaturesByCity.csv' data.                                               ###\n",
    "### 2. As the date is huge writing partitiong is by country and city and writing into parquet format  ###\n",
    "#########################################################################################################\n",
    "\n",
    "global_temp_df = spark.read.csv('../../data2/GlobalLandTemperaturesByCity.csv',header=True)\n",
    "global_temp_df.write.mode(\"overwrite\").partitionBy(\"country\",\"city\").parquet(\"Datasets_created_by_Scripts/global_temperatures\")\n",
    "global_tempetature_df = spark.read.parquet(\"Datasets_created_by_Scripts/global_temperatures\")\n",
    "\n",
    "#global_tempetature_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|     US-KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|     US-AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+------------------+-----+\n",
      "|         City|        State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|              Race|Count|\n",
      "+-------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+------------------+-----+\n",
      "|Silver Spring|     Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|Hispanic or Latino|25924|\n",
      "|       Quincy|Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|             White|58723|\n",
      "|       Hoover|      Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|             Asian| 4759|\n",
      "+-------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------+------------------+-----------------------------+--------+---------+-------------+-----------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty|Latitude|Longitude|      country|       city|\n",
      "+----------+------------------+-----------------------------+--------+---------+-------------+-----------+\n",
      "|1758-03-01| 6.422999999999999|                        3.742|  37.78N|   93.56W|United States|Springfield|\n",
      "|1758-04-01|             12.14|                        5.432|  37.78N|   93.56W|United States|Springfield|\n",
      "|1758-05-01|16.997999999999998|                         3.76|  37.78N|   93.56W|United States|Springfield|\n",
      "+----------+------------------+-----------------------------+--------+---------+-------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_codes_df.show(3)\n",
    "us_cities_df.show(3)\n",
    "global_tempetature_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Gathering Data\n",
    "In the above immigration data there are 28 columns out of which 7 are *I94* columns.\n",
    "  1. **I94YR**   - Year\n",
    "  2. **I94MON**  - Month \n",
    "  3. **I94CIT**  - City Codes\n",
    "  4. **I94RES**  - Residence Codes\n",
    "  5. **I94PORT** - PORT Codes\n",
    "  6. **I94MODE** - Mode of Immigration Codes\n",
    "  7. **I94VISA** - VISA Codes\n",
    "\n",
    "Other than I94YR and I94MON all other I94 columns are populated with respective codes. The descriptions for these codes are present in  _`I94_SAS_Label_Descriptors.SAS`_ file.\n",
    "\n",
    "##### Labels and Codes from _I94_SAS_Label_Descriptors.SAS_ file\n",
    "Taking the _`I94_SAS_Label_Descriptors.SAS`_ I have  created a  **nested** json file - `I94_SAS_Label_Descriptors.json`. <br> \n",
    "File coversion is done using  `class LabelDescriptorToJson` from **`LabelDescriptorToJson.py`** script.\n",
    "      \n",
    "      class takes 3 arguments as inputs:\n",
    "          input_file_path - path to `I94_SAS_Label_Descriptors.SAS` including filename\n",
    "          labels - all the `I94` columns names that are coded\n",
    "          output_file_path - output json file path with filename\n",
    "\n",
    "Invoking class : **`LabelDescriptorToJson(input_file,labels,output_file)`**\n",
    "\n",
    "Following is the JSON file structure created by script\n",
    "           \n",
    "   ```json\n",
    "   [\n",
    "    {\n",
    "      \"I94CIT_I94RES\": [{\"key\": \"582\", \"val\": \"MEXICO Air Sea, and Not Reported (I-94, no land arrivals)\"}, \n",
    "                    {\"key\": \"236\", \"val\": \"AFGHANISTAN\"}, {\"key\": \"101\", \"val\": \"ALBANIA\"},......]\n",
    "    },\n",
    "    {\n",
    "      \"I94PORT\": [{\"key\": \"ALC\", \"val\": \"ALCAN, AK\"}, {\"key\": \"ANC\", \"val\": \"ANCHORAGE, AK\"},... ]\n",
    "    },\n",
    "    {\n",
    "      \"I94MODE\": [{\"key\": \"1\", \"val\": \"Air\"}, {\"key\": \"2\", \"val\": \"Sea\"}, {\"key\": \"3\", \"val\": \"Land\"}, {\"key\": \"9\", \"val\": \"Not reported\"}]\n",
    "    }, \n",
    "    {\n",
    "      \"I94ADDR\": [{\"key\": \"AL\", \"val\": \"ALABAMA\"}, {\"key\": \"AK\", \"val\": \"ALASKA\"}, {\"key\": \"AZ\", \"val\": \"ARIZONA\"}, ...]\n",
    "    }, \n",
    "    {\n",
    "      \"I94VISA\": [{\"key\": \"1\", \"val\": \"Business\"}, {\"key\": \"2\", \"val\": \"Pleasure\"}, {\"key\": \"3\", \"val\": \"Student\"}]\n",
    "    }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Creating SAS Label Descriptor JSON file at : Datasets_created_by_Scripts/I94_SAS_Label_Descriptors.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<LabelDescriptorToJson.LabelDescriptorToJson at 0x7fa0bd1ca0b8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####\n",
    "#### Importing LabelDescriptorToJson class and creating JSON file from the I94_SAS_Labels_Descriptions.SAS file\n",
    "####\n",
    "from LabelDescriptorToJson import LabelDescriptorToJson\n",
    "inpt_file = 'Datasets/I94_SAS_Labels_Descriptions.SAS'\n",
    "labels = ['I94CIT', 'I94RES','I94PORT','I94MODE','I94ADDR','I94VISA']\n",
    "opt_file = 'Datasets_created_by_Scripts/I94_SAS_Label_Descriptors.json'\n",
    "LabelDescriptorToJson(inpt_file,labels,opt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identifying data quality issues, like missing values, duplicate data, etc. <br>\n",
    "\n",
    "Reading the JSON data created in the above steps and exploding the I94 columns seperately to find the relations with other datasets -> us-cities-demographics.csv, airport-codes_csv.csv, GlobalLandTemperaturesByCity.csv \n",
    "\n",
    "#### Cleaning Steps\n",
    "1. Selecting only the required columns from the immigration table.\n",
    "2. Filtering the data by 'Air' mode i.e., i94mode = 1. Following are three modes of immigrations to US \n",
    "    * 1 = 'Air'\n",
    "    * 2 = 'Sea'\n",
    "    * 3 = 'Land'\n",
    "    * 9 = 'Not reported'\n",
    "   \n",
    "3. Selecting necessary columns from Datasets/us-cities-demographics.csv\n",
    "4. Since we are dealing with only US immigration data filtering the `/data/data2/GlobalLandTemperaturesByCity.csv` by `country = 'United States'`\n",
    "5. Filtering the global tempetatures dataset by removing `null` temperature values i.e., AverageTemperature AND AverageTemperatureUncertainty with null values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "####################################################################################\n",
    "####                                                                            ####\n",
    "#### Reading the json data and exploding each and every  I94 columns seperetely ####\n",
    "####                                                                            #### \n",
    "####################################################################################\n",
    "####################################################################################\n",
    "label_desc_df = spark.read.json(\"Datasets_created_by_Scripts/I94_SAS_Label_Descriptors.json\")\n",
    "\n",
    "\n",
    "modes_df =label_desc_df.select(explode(col(\"I94MODE\"))).toDF(\"mode\").selectExpr(\"CAST(mode.key AS INT) mode_id\",\"mode.val as mode_name\") # MODES DIMENSION TABLE\n",
    "addr_df = label_desc_df.select(explode(col(\"I94ADDR\"))).toDF(\"addr\").selectExpr(\"addr.key as state_code\",\"addr.val as state\")  # STATES TABLE\n",
    "city_res_df = label_desc_df.select(explode(col(\"I94CIT_I94RES\"))).toDF(\"city_res\").selectExpr(\"CAST(city_res.key AS INT) city_code\",\"city_res.val as city\") # CITIES DIMENSION TABLE\n",
    "ports_df = label_desc_df.select(explode(col(\"I94PORT\"))).toDF(\"port\").selectExpr(\"port.key as port_code\",\"port.val as port_name\") # PORTS TABLE\n",
    "visa_code_df = label_desc_df.select(explode(col(\"I94VISA\"))).toDF(\"visa\").selectExpr(\"CAST(visa.key AS INT) visa_id\",\"visa.val as visa_name\") # VISA DIMENSON TABLE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- mode_id: integer (nullable = true)\n",
      " |-- mode_name: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- city_code: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- port_code: string (nullable = true)\n",
      " |-- port_name: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- visa_id: integer (nullable = true)\n",
      " |-- visa_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "### Following sample datasets and schema for thr SAS Descriptors which will be loaded into 4 Dimension tables\n",
    "###\n",
    "\n",
    "#modes_df.show()      # MODES DIMENSION TABLE\n",
    "#addr_df.show(3)      # STATES DIMENSION TABLE\n",
    "#city_res_df.show(3)  # CITIES DIMENSION TABLE\n",
    "#ports_df.show(3)     # PORTS DIMENSION TABLE\n",
    "#visa_code_df.show(3) # VISA DIMENSON TABLE\n",
    "\n",
    "\n",
    "modes_df.printSchema()      # MODES DIMENSION TABLE\n",
    "addr_df.printSchema()      # STATES DIMENSION TABLE\n",
    "city_res_df.printSchema()  # CITIES DIMENSION TABLE\n",
    "ports_df.printSchema()     # PORTS DIMENSION TABLE\n",
    "visa_code_df.printSchema() # VISA DIMENSON TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "### Following are some of the invalid Codes present in the immigration data\n",
    "###\n",
    "#city_res_df.select(['city_res_code','value']).where(\"value like '%INVALID%'\").show()\n",
    "#airport_codes_df.select(['airport_code','value']).where(\"value like '%INVALID%'\").show()\n",
    "#airport_codes_df.select(['airport_code','value']).where(\"value like 'No PORT Code%'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+------+------+\n",
      "|    cicid|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype| i94yr|i94mon|\n",
      "+---------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+------+------+\n",
      "|5748517.0| 245.0| 438.0|    LOS|20574.0|    1.0|     CA|20582.0|  40.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1|2016.0|   4.0|\n",
      "|5748518.0| 245.0| 438.0|    LOS|20574.0|    1.0|     NV|20591.0|  32.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1984.0|10292016|     F|  null|     VA|9.495562283E10|00007|      B1|2016.0|   4.0|\n",
      "|5748519.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20582.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     M|  null|     DL|9.495640653E10|00040|      B1|2016.0|   4.0|\n",
      "|5748520.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     F|  null|     DL|9.495645143E10|00040|      B1|2016.0|   4.0|\n",
      "|5748521.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  28.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1988.0|10292016|     M|  null|     DL|9.495638813E10|00040|      B1|2016.0|   4.0|\n",
      "+---------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Performing cleaning tasks here\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "###                                                                                      ###\n",
    "### Exploring Immigration data read from 'i94_apr16_sub.sas7bdat'  into 'immigration_df' ###\n",
    "###                                                                                      ###\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "\n",
    "# already read before\n",
    "# immigration_df=spark.read.parquet(\"Datasets_created_by_Scripts/sas_data\")\n",
    "\n",
    "###\n",
    "### Filtering the data with immigration mode of Air\n",
    "###\n",
    "immigration_df = immigration_df.filter(col('i94mode')== 1)\n",
    "immigration_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#############################################################################################################################\n",
    "#############################################################################################################################\n",
    "######## udf definition to convert the sas date numeric to DateType()                                                ########\n",
    "########                                                                                                             ########\n",
    "######## SAS date value is a value that represents the number of days between January 1, 1960, and a specified date. ########\n",
    "######## Therefore, if we convert the numbers to Pandas Timedeltas and add them to 1960-1-1 we can recover the date  ########\n",
    "#############################################################################################################################\n",
    "#############################################################################################################################\n",
    "\n",
    "### convert_sas_date = udf(lambda epoch: pd.to_timedelta(epoch, unit='D') + pd.datetime(1960, 1, 1),DateType())\n",
    "\n",
    "@udf(DateType())\n",
    "def convert_sas_date(epoch):\n",
    "    if epoch:\n",
    "        time = pd.to_timedelta(epoch, unit='D') + pd.datetime(1960, 1, 1)\n",
    "        return time\n",
    "    else:\n",
    "        return datetime(9999,9,9)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "### Selecting only the required columns\n",
    "###\n",
    "df = immigration_df.selectExpr([\"CAST(cicid AS BIGINT) \",\n",
    "                                    \"CAST(i94yr AS INT) \",\n",
    "                                    \"CAST(i94mon AS INT) \",\n",
    "                                    \"CAST(i94cit AS INT)  \",\n",
    "                                    \"CAST(i94res AS INT) \",\n",
    "                                    \"i94port\",\n",
    "                                    \"CAST(arrdate AS BIGINT) arrival_date \",\n",
    "                                    \"CAST(i94mode as INT) \",\n",
    "                                    \"i94addr\",\n",
    "                                    \"CAST(depdate AS BIGINT) departure_date \",\n",
    "                                    \"CAST(i94visa AS INT)\",\n",
    "                                    \"CAST(biryear AS INT) \",\n",
    "                                    \"gender\",\n",
    "                                    \"visatype\"])\n",
    "\n",
    "\n",
    "###\n",
    "### following .withColumn will overwrite the sas date numberic with dateType() as the given alias column name is same as the inputcolumn name i.e., arrival_date and departure_date\n",
    "###\n",
    "immig_df = df.withColumn(\"arrival_date\",convert_sas_date(\"arrival_date\")).withColumn(\"departure_date\",convert_sas_date(\"departure_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+------+------+-------+------------+-------+-------+--------------+-------+-------+------+--------+\n",
      "|  cicid|i94yr|i94mon|i94cit|i94res|i94port|arrival_date|i94mode|i94addr|departure_date|i94visa|biryear|gender|visatype|\n",
      "+-------+-----+------+------+------+-------+------------+-------+-------+--------------+-------+-------+------+--------+\n",
      "|5903213| 2016|     4|   582|   582|    AUS|  2016-04-10|      1|     TX|    9999-09-09|      2|   null|     M|      B2|\n",
      "|5904031| 2016|     4|   131|   131|    SDP|  2016-04-11|      1|   null|    2016-04-12|      2|   null|     U|      WT|\n",
      "|5905191| 2016|     4|   180|   135|    SDP|  2016-04-11|      1|   null|    9999-09-09|      2|   null|     F|      WT|\n",
      "+-------+-----+------+------+------+-------+------------+-------+-------+--------------+-------+-------+------+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------+-----+------+------+------+-------+------------+-------+-------+--------------+-------+-------+------+--------+\n",
      "|  cicid|i94yr|i94mon|i94cit|i94res|i94port|arrival_date|i94mode|i94addr|departure_date|i94visa|biryear|gender|visatype|\n",
      "+-------+-----+------+------+------+-------+------------+-------+-------+--------------+-------+-------+------+--------+\n",
      "|5748528| 2016|     4|   245|   504|    LOS|  2016-04-30|      1|   null|    2016-05-01|      2|   1977|     M|      B2|\n",
      "|5748918| 2016|     4|   249|   249|    TOR|  2016-04-29|      1|   null|    2016-05-08|      2|   1985|     F|      B2|\n",
      "|5748922| 2016|     4|   249|   249|    NYC|  2016-04-30|      1|   null|    2016-05-18|      2|   1963|     M|      B2|\n",
      "+-------+-----+------+------+------+-------+------------+-------+-------+--------------+-------+-------+------+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----+-----+------+------+------+-------+------------+-------+-------+--------------+-------+-------+------+--------+\n",
      "|cicid|i94yr|i94mon|i94cit|i94res|i94port|arrival_date|i94mode|i94addr|departure_date|i94visa|biryear|gender|visatype|\n",
      "+-----+-----+------+------+------+-------+------------+-------+-------+--------------+-------+-------+------+--------+\n",
      "+-----+-----+------+------+------+-------+------------+-------+-------+--------------+-------+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "### Checking for any null columns\n",
    "###\n",
    "\n",
    "immig_df.filter(\"i94cit IS NULL or i94res IS NULL  or i94mode IS NULL  or i94visa IS NULL or biryear IS NULL\").show(3)\n",
    "immig_df.filter(\"i94port IS NULL or i94addr IS NULL or gender IS NULL\").show(3)\n",
    "immig_df.filter(\"visatype IS NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+------+------+-------+------------+-------+-------+--------------+-------+-------+------+--------+\n",
      "|  cicid|i94yr|i94mon|i94cit|i94res|i94port|arrival_date|i94mode|i94addr|departure_date|i94visa|biryear|gender|visatype|\n",
      "+-------+-----+------+------+------+-------+------------+-------+-------+--------------+-------+-------+------+--------+\n",
      "|5748517| 2016|     4|   245|   438|    LOS|  2016-04-30|      1|     CA|    2016-05-08|      1|   1976|     F|      B1|\n",
      "|5748518| 2016|     4|   245|   438|    LOS|  2016-04-30|      1|     NV|    2016-05-17|      1|   1984|     F|      B1|\n",
      "|5748519| 2016|     4|   245|   438|    LOS|  2016-04-30|      1|     WA|    2016-05-08|      1|   1987|     M|      B1|\n",
      "|5748520| 2016|     4|   245|   438|    LOS|  2016-04-30|      1|     WA|    2016-05-14|      1|   1987|     F|      B1|\n",
      "|5748521| 2016|     4|   245|   438|    LOS|  2016-04-30|      1|     WA|    2016-05-14|      1|   1988|     M|      B1|\n",
      "|5748522| 2016|     4|   245|   464|    HHW|  2016-04-30|      1|     HI|    2016-05-05|      2|   1959|     M|      B2|\n",
      "|5748523| 2016|     4|   245|   464|    HHW|  2016-04-30|      1|     HI|    2016-05-12|      2|   1950|     F|      B2|\n",
      "|5748524| 2016|     4|   245|   464|    HHW|  2016-04-30|      1|     HI|    2016-05-12|      2|   1975|     F|      B2|\n",
      "|5748525| 2016|     4|   245|   464|    HOU|  2016-04-30|      1|     FL|    2016-05-07|      2|   1989|     M|      B2|\n",
      "|5748526| 2016|     4|   245|   464|    LOS|  2016-04-30|      1|     CA|    2016-05-07|      2|   1990|     F|      B2|\n",
      "|5748527| 2016|     4|   245|   504|    NEW|  2016-04-30|      1|     MA|    2016-05-02|      2|   1972|     M|      B2|\n",
      "|5748528| 2016|     4|   245|   504|    LOS|  2016-04-30|      1|NO ADDR|    2016-05-01|      2|   1977|     M|      B2|\n",
      "|5748529| 2016|     4|   245|   504|    WAS|  2016-04-30|      1|     VA|    2016-05-22|      2|   1978|     M|      B2|\n",
      "|5748530| 2016|     4|   245|   504|    LOS|  2016-04-30|      1|     CA|    2016-05-03|      2|   1960|     F|      B2|\n",
      "|5748531| 2016|     4|   245|   504|    LOS|  2016-04-30|      1|     CA|    2016-05-03|      2|   1978|     M|      B2|\n",
      "|5748532| 2016|     4|   245|   504|    MIA|  2016-04-30|      1|     FL|    2016-05-07|      2|   1963|     F|      B2|\n",
      "|5748534| 2016|     4|   245|   528|    SFR|  2016-04-30|      1|     CA|    9999-09-09|      2|   1932|     F|      B2|\n",
      "|5748876| 2016|     4|   245|   582|    HOU|  2016-04-30|      1|     TX|    2016-05-09|      1|   1973|     M|      B1|\n",
      "|5748877| 2016|     4|   245|   582|    HOU|  2016-04-30|      1|     TX|    2016-05-09|      1|   1986|     F|      B1|\n",
      "|5748881| 2016|     4|   245|   582|    LOS|  2016-04-30|      1|     CA|    2016-05-01|      2|   1982|     M|      B2|\n",
      "+-------+-----+------+------+------+-------+------------+-------+-------+--------------+-------+-------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "### Replacing Null columns with Default values\n",
    "###\n",
    "col_defaults = {\n",
    "                 \"gender\" :'NA',\n",
    "                 \"i94addr\":'NO ADDR',\n",
    "                 \"biryear\" : -999999\n",
    "}\n",
    "\n",
    "cleansed_immig_df = immig_df.na.fill(col_defaults)\n",
    "clean_immig_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|state_code|     state|\n",
      "+----------+----------+\n",
      "|        AL|   ALABAMA|\n",
      "|        AK|    ALASKA|\n",
      "|        AZ|   ARIZONA|\n",
      "|        AR|  ARKANSAS|\n",
      "|        CA|CALIFORNIA|\n",
      "+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|            City|        State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race|Count|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|   Silver Spring|     Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino|25924|\n",
      "|          Quincy|Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White|58723|\n",
      "|          Hoover|      Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian| 4759|\n",
      "|Rancho Cucamonga|   California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...|24437|\n",
      "|          Newark|   New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White|76402|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "### DF of exploded 'I94addr' from SAS Json file \n",
    "###\n",
    "\n",
    "addr_df.show(5)  # STATES DIMENSION TABLE\n",
    "\n",
    "#us_cities_df = spark.read.csv(\"Datasets/us-cities-demographics.csv\",sep = \";\",header = True) #This DF was run in Step 1 so no need to run again\n",
    "\n",
    "#################################################################\n",
    "#################################################################\n",
    "####                                                         ####\n",
    "#### Exploring and cleaning  us-cities-demographics.csv data ####\n",
    "####                                                         ####\n",
    "#################################################################\n",
    "#################################################################\n",
    "\n",
    "us_cities_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "### From the above I94 Codes we can relate 'addr_codes' with 'state_codes' in demographics\n",
    "###\n",
    "df = us_cities_df.selectExpr([\"City as city\",\n",
    "                                          \"State as state\",\n",
    "                                          \"CAST(`Median Age` AS DOUBLE) median_age\",\n",
    "                                          \"CAST(`Male Population` AS INT) male_population\",\n",
    "                                          \"CAST(`Female Population` AS INT) female_population\",\n",
    "                                          \"CAST(`Total Population` AS INT) total_population\",\n",
    "                                         \"CAST(`Number of Veterans` AS INT) as veterans_count\",\n",
    "                                         \"CAST(`Foreign-born` AS INT) foriegn_born\",\n",
    "                                          \"Race\",\n",
    "                                          \"`State code` AS state_code\"\n",
    "                                         ])\n",
    "\n",
    "#cleansed_us_cities_df.show(5)\n",
    "\n",
    "####\n",
    "#### US_CITY_DEMOGRAPHICS TABLE\n",
    "####\n",
    "#### us-cities-demographics can be connected by 'state_code' with 'STATE_CODE' column STATES DIMENSION TABLE\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+---------------+-----------------+----------------+--------------+------------+--------------------+----------+\n",
      "|    city|      state|median_age|male_population|female_population|total_population|veterans_count|foriegn_born|                Race|state_code|\n",
      "+--------+-----------+----------+---------------+-----------------+----------------+--------------+------------+--------------------+----------+\n",
      "|San Juan|Puerto Rico|      41.4|         155408|           186829|          342237|          null|        null|  Hispanic or Latino|        PR|\n",
      "|  Caguas|Puerto Rico|      40.4|          34743|            42265|           77008|          null|        null|  Hispanic or Latino|        PR|\n",
      "|Carolina|Puerto Rico|      42.0|          64758|            77308|          142066|          null|        null|American Indian a...|        PR|\n",
      "|Carolina|Puerto Rico|      42.0|          64758|            77308|          142066|          null|        null|  Hispanic or Latino|        PR|\n",
      "|San Juan|Puerto Rico|      41.4|         155408|           186829|          342237|          null|        null|American Indian a...|        PR|\n",
      "+--------+-----------+----------+---------------+-----------------+----------------+--------------+------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----+-----+----------+---------------+-----------------+----------------+--------------+------------+----+----------+\n",
      "|city|state|median_age|male_population|female_population|total_population|veterans_count|foriegn_born|Race|state_code|\n",
      "+----+-----+----------+---------------+-----------------+----------------+--------------+------------+----+----------+\n",
      "+----+-----+----------+---------------+-----------------+----------------+--------------+------------+----+----------+\n",
      "\n",
      "+--------+-----------+----------+---------------+-----------------+----------------+--------------+------------+--------------------+----------+\n",
      "|    city|      state|median_age|male_population|female_population|total_population|veterans_count|foriegn_born|                Race|state_code|\n",
      "+--------+-----------+----------+---------------+-----------------+----------------+--------------+------------+--------------------+----------+\n",
      "|San Juan|Puerto Rico|      41.4|         155408|           186829|          342237|          -999|        -999|  Hispanic or Latino|        PR|\n",
      "|  Caguas|Puerto Rico|      40.4|          34743|            42265|           77008|          -999|        -999|  Hispanic or Latino|        PR|\n",
      "|Carolina|Puerto Rico|      42.0|          64758|            77308|          142066|          -999|        -999|American Indian a...|        PR|\n",
      "|Carolina|Puerto Rico|      42.0|          64758|            77308|          142066|          -999|        -999|  Hispanic or Latino|        PR|\n",
      "|San Juan|Puerto Rico|      41.4|         155408|           186829|          342237|          -999|        -999|American Indian a...|        PR|\n",
      "+--------+-----------+----------+---------------+-----------------+----------------+--------------+------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "### Setting default values for the null columns\n",
    "###\n",
    "df.filter(\"foriegn_born IS NULL\").show(5)\n",
    "cleansed_us_cities_df = df.na.fill(-999,[\"veterans_count\",\"foriegn_born\"])\n",
    "cleansed_us_cities_df.filter(\"foriegn_born IS NULL\").show(5)\n",
    "cleansed_us_cities_df.filter(\"foriegn_born = -999\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# already read before\n",
    "#global_tempetature_df = spark.read.parquet(\"Datasets_created_by_Scripts/global_temperatures\")\n",
    "\n",
    "########################################################################\n",
    "########################################################################\n",
    "#####                                                              #####\n",
    "##### Exploring and cleaning GlobalLandTemperaturesByCity.csv data #####\n",
    "#####                                                              #####\n",
    "########################################################################\n",
    "########################################################################\n",
    "\n",
    "global_tempetature_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#United States\n",
    "###\n",
    "### Creating temporary view for global_tempetature_df\n",
    "###\n",
    "global_tempetature_df.createOrReplaceTempView(\"globalTempTable\")\n",
    "cleansed_us_cities_df.createOrReplaceTempView(\"us_demographics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|US_TemperaturesCount|\n",
      "+--------------------+\n",
      "|              687289|\n",
      "+--------------------+\n",
      "\n",
      "+-------------------------------+\n",
      "|US_TemperaturesWithNull_records|\n",
      "+-------------------------------+\n",
      "|                          25765|\n",
      "+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "#### Checking for the Null temperature records\n",
    "####\n",
    "spark.sql(\"\"\"SELECT count(1) US_TemperaturesCount \n",
    "                    FROM globalTempTable \n",
    "                    WHERE Country = 'United States'\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"SELECT count(1) US_TemperaturesWithNull_records\n",
    "                    FROM globalTempTable \n",
    "                    WHERE AverageTemperature IS NULL AND AverageTemperatureUncertainty IS NULL \n",
    "                            AND Country = 'United States'\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------------------+-----------+-------------+--------+---------+\n",
      "|noted_date|Average_Temperature|Average_Temperature_Uncertainty|       City|      Country|Latitude|Longitude|\n",
      "+----------+-------------------+-------------------------------+-----------+-------------+--------+---------+\n",
      "|1758-03-01|  6.422999999999999|                          3.742|Springfield|United States|  37.78N|   93.56W|\n",
      "|1758-04-01|              12.14|                          5.432|Springfield|United States|  37.78N|   93.56W|\n",
      "|1758-05-01| 16.997999999999998|                           3.76|Springfield|United States|  37.78N|   93.56W|\n",
      "|1758-06-01|             22.852|             3.5189999999999997|Springfield|United States|  37.78N|   93.56W|\n",
      "|1758-07-01|             25.195|                          4.888|Springfield|United States|  37.78N|   93.56W|\n",
      "+----------+-------------------+-------------------------------+-----------+-------------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- noted_date: date (nullable = true)\n",
      " |-- Average_Temperature: double (nullable = true)\n",
      " |-- Average_Temperature_Uncertainty: double (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "#### Cleaning the GlobalLandTemperaturesByCity.csv data and selecting only 'United States' records\n",
    "####\n",
    "\n",
    "cleansed_global_temp_df = spark.sql(\"\"\"SELECT TO_DATE(dt) as noted_date,\n",
    "                                                CAST(AverageTemperature AS DOUBLE) Average_Temperature,\n",
    "                                                CAST(AverageTemperatureUncertainty AS DOUBLE) Average_Temperature_Uncertainty,   \n",
    "                                                City,      \n",
    "                                                Country,\n",
    "                                                Latitude,\n",
    "                                                Longitude FROM globalTempTable \n",
    "                                                WHERE AverageTemperature IS NOT NULL \n",
    "                                                AND AverageTemperatureUncertainty IS NOT NULL  \n",
    "                                                AND Country = 'United States'\"\"\")\n",
    "cleansed_global_temp_df.show(5)\n",
    "\n",
    "#####\n",
    "##### TEMPERATURES TABLE\n",
    "#####\n",
    "##### CITY column of globalTempperature data can be joined CITY of US_CITY_DEMOGRAPHICS table\n",
    "#####\n",
    "cleansed_global_temp_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# already read before\n",
    "# airport_codes_df = spark.read.csv(\"Datasets/airport-codes_csv.csv\",header = True)\n",
    "\n",
    "###################################################\n",
    "###################################################\n",
    "#####                                         #####\n",
    "##### Exploring airport-codes_csv.csv dataset #####\n",
    "#####                                         #####\n",
    "###################################################\n",
    "###################################################\n",
    "\n",
    "###\n",
    "### AIRPORTS TABLE\n",
    "### airport_codes can be connected by 'local_code' with 'PORT_CODE' column PORTS DIMENSION TABLE\n",
    "###\n",
    "\n",
    "\n",
    "clean_airport_codes_df = airport_codes_df.filter(col(\"iso_country\")== 'US')\n",
    "\n",
    "clean_airport_codes_df = clean_airport_codes_df.selectExpr(\"ident\",\n",
    "                                                \"type\",\n",
    "                                                \"name\",\n",
    "                                                \"CAST(elevation_ft as INT)\",\n",
    "                                                \"iso_country\",\n",
    "                                                \"iso_region\",\n",
    "                                                \"municipality\",\n",
    "                                                \"gps_code\",\n",
    "                                                \"local_code\",\n",
    "                                                \"coordinates\")\n",
    "clean_airport_codes_df.printSchema() \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Following is the Conceptual model  that can be formed by exploring the above datasets\n",
    "\n",
    "<img src = \"./images/Immigration_CONCEPTUAL_Schema.jpg\">\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "To load the data into above modelled table following are the steps:\n",
    "1. As the data is huge first load the data files on to S3 bucket\n",
    "2. Processing in them using spark would be easy considering is Schema on read property and the size of the data we are dealing with.\n",
    "3. Processed data are to be loaded into Redshift cluster using postgres jdbc connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "### CREATING TEMPORARY VIEWS FOR RUNNING SOME QUERIES\n",
    "###\n",
    "airport_codes_df.createOrReplaceTempView(\"airport_codes_table\")\n",
    "ports_df.createOrReplaceTempView(\"ports_table\")\n",
    "cleansed_immig_df.createOrReplaceTempView(\"immigration_table\")\n",
    "addr_df.createOrReplaceTempView(\"states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "All the above code snippets in the Jupyter notebook are used in the following scripts.\n",
    "Following are the scripts to build the above mentioned piepline\n",
    "   1. LoadingDataFilesToS3.py - Before loading on to S3, `LabelDescriptorToJson` class from LabelDescriptorToJson.py is called to convert the SAS labels into JSON file\n",
    "   2. CreateTables.py - Creates Tables on Redshift - Uses sql_queries.py which contains CREATE and DROP statements\n",
    "   3. ETL2Redshift.py - Processes the data and load into Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This script shoud be run on EMR\n",
    "%run LoadingDataFilesToS3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Redshift Cluster...\n",
      "Connected to Redshift Cluster...\n",
      "'MODES' table created successfully...!!!\n",
      "'VISAS' table created successfully...!!!\n",
      "'PORTS' table created successfully...!!!\n",
      "'STATES' table created successfully...!!!\n",
      "'CITIES' table created successfully...!!!\n",
      "'AIRPORTS' table created successfully...!!!\n",
      "'US_CITY_DEMOGRAPHICS' table created successfully...!!!\n",
      "'TEMPERATURES' table created successfully...!!!\n",
      "'IMMIGRANTS' table created successfully...!!!\n",
      "******* SCRIPT COMPLETED *******\n"
     ]
    }
   ],
   "source": [
    "%run CreateTables.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This should be run on EMR cluster as the process takes too long\n",
    "%run ETL2Redshift.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here\n",
    "#connects to you  Redshift Cluster\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('cloud.cfg'))\n",
    "(HOST,DB_NAME, DB_USER, DB_PASSWORD, DB_PORT) = config['CLUSTER'].values()\n",
    "\n",
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DB_USER, DB_PASSWORD, HOST, DB_PORT,DB_NAME)\n",
    "print(conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i94mode</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2994505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   i94mode    count\n",
       "0        1  2994505"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql \n",
    "SELECT i94mode,COUNT(*) FROM IMMIGRANTS WHERE i94mode=1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visa_id</th>\n",
       "      <th>visa_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Pleasure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Student</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   visa_id visa_name\n",
       "0        1  Business\n",
       "1        2  Pleasure\n",
       "2        3   Student"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT * from visas;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>median_age</th>\n",
       "      <th>male_population</th>\n",
       "      <th>female_population</th>\n",
       "      <th>total_population</th>\n",
       "      <th>veterans_count</th>\n",
       "      <th>foriegn_born</th>\n",
       "      <th>Race</th>\n",
       "      <th>state_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601</td>\n",
       "      <td>41862</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562</td>\n",
       "      <td>30908</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>MD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129</td>\n",
       "      <td>49500</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147</td>\n",
       "      <td>32935</td>\n",
       "      <td>White</td>\n",
       "      <td>MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040</td>\n",
       "      <td>46799</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819</td>\n",
       "      <td>8229</td>\n",
       "      <td>Asian</td>\n",
       "      <td>AL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127</td>\n",
       "      <td>87105</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821</td>\n",
       "      <td>33878</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040</td>\n",
       "      <td>143873</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829</td>\n",
       "      <td>86253</td>\n",
       "      <td>White</td>\n",
       "      <td>NJ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               city          state  median_age  male_population  \\\n",
       "0     Silver Spring       Maryland        33.8            40601   \n",
       "1            Quincy  Massachusetts        41.0            44129   \n",
       "2            Hoover        Alabama        38.5            38040   \n",
       "3  Rancho Cucamonga     California        34.5            88127   \n",
       "4            Newark     New Jersey        34.6           138040   \n",
       "\n",
       "   female_population  total_population  veterans_count  foriegn_born  \\\n",
       "0              41862             82463            1562         30908   \n",
       "1              49500             93629            4147         32935   \n",
       "2              46799             84839            4819          8229   \n",
       "3              87105            175232            5821         33878   \n",
       "4             143873            281913            5829         86253   \n",
       "\n",
       "                        Race state_code  \n",
       "0         Hispanic or Latino         MD  \n",
       "1                      White         MA  \n",
       "2                      Asian         AL  \n",
       "3  Black or African-American         CA  \n",
       "4                      White         NJ  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT * from US_CITY_DEMOGRAPHICS;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count(1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count(1)\n",
       "0     55075"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "Select count(*) from AIRPORTS;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>port_name</th>\n",
       "      <th>name</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PINECREEK BORDER ARPT, MN</td>\n",
       "      <td>Piney Pinecreek Border Airport</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-MN</td>\n",
       "      <td>Pinecreek</td>\n",
       "      <td>48Y</td>\n",
       "      <td>-95.98259735107422, 48.99959945678711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KETCHIKAN, AK</td>\n",
       "      <td>Ketchikan Harbor Seaplane Base</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Ketchikan</td>\n",
       "      <td>5KE</td>\n",
       "      <td>-131.677002, 55.349899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CRANE LAKE - ST. LOUIS CNTY, NM</td>\n",
       "      <td>Scotts Seaplane Base</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-MN</td>\n",
       "      <td>Crane Lake</td>\n",
       "      <td>CDD</td>\n",
       "      <td>-92.483497619629, 48.2666015625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CASPER, WY</td>\n",
       "      <td>Cape Spencer C.G. Heliport</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Cape Spencer</td>\n",
       "      <td>CSP</td>\n",
       "      <td>-136.639007568, 58.19910049439999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DONNA, TX</td>\n",
       "      <td>DoÃ±a Ana County International Jetport</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-NM</td>\n",
       "      <td>Santa Teresa</td>\n",
       "      <td>DNA</td>\n",
       "      <td>-106.705002, 31.881001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         port_name                                    name  \\\n",
       "0        PINECREEK BORDER ARPT, MN          Piney Pinecreek Border Airport   \n",
       "1                    KETCHIKAN, AK          Ketchikan Harbor Seaplane Base   \n",
       "2  CRANE LAKE - ST. LOUIS CNTY, NM                    Scotts Seaplane Base   \n",
       "3                       CASPER, WY              Cape Spencer C.G. Heliport   \n",
       "4                        DONNA, TX  DoÃ±a Ana County International Jetport   \n",
       "\n",
       "  continent iso_country iso_region  municipality local_code  \\\n",
       "0        NA          US      US-MN     Pinecreek        48Y   \n",
       "1        NA          US      US-AK     Ketchikan        5KE   \n",
       "2        NA          US      US-MN    Crane Lake        CDD   \n",
       "3        NA          US      US-AK  Cape Spencer        CSP   \n",
       "4        NA          US      US-NM  Santa Teresa        DNA   \n",
       "\n",
       "                             coordinates  \n",
       "0  -95.98259735107422, 48.99959945678711  \n",
       "1                 -131.677002, 55.349899  \n",
       "2        -92.483497619629, 48.2666015625  \n",
       "3      -136.639007568, 58.19910049439999  \n",
       "4                 -106.705002, 31.881001  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT p.port_name,name,\n",
    "        continent,\n",
    "        iso_country,\n",
    "        iso_region,\n",
    "        municipality,\n",
    "        local_code,\n",
    "        coordinates FROM AIRPORTS a \n",
    "        JOIN PORTS p ON a.local_code = p.port_code  \n",
    "WHERE iso_country = 'US' LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Th data Disctionary is provired in a seperate file - `DataDictionary`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* I have used Apacke Spark to read, Transform and Load the data.While working with huge volumes of data spark processing speed is high and data manipulation and cleansing though dtataframes is very easy.\n",
    "* Data of the immigration departments will be high. To get the recent data for analysis updating it once in a Month would be prefereble, so that analysis for how much inflow to the country is increased or decreased an be kept track.\n",
    "* If the data is increased by 100x I would run my scripts though a cluster though using Amazon EMR. an dLoad it into Redshift so that it can be accessed by 100+ people\n",
    "* Automating cexecution order of the scripts can be done though scheduling tools. Assuming this huge data to be processed, ETL Job can be scheduled daily at 7pm so that data is updated by 7am every day."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
